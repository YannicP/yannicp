## Some of my Projects:

This document gives some insights into some of my past projects, with a focus on variety.

### Contents:
You can click the project to jump straight to the section.

1. [Deriving Strategic Targets by Analyzing Amazon Customer Reviews](#p1_link) (R, text mining, Word2Vec, topic modeling, LDA)  
2. [Using Computer Vision and Machine Learning to Identify Material Defects](#p2_link) (Matlab, Image Transformations, CNN, Transfer Learning, One-Class-SVM)
3. [Solving the Kaggle Titanic Challenge using Tensorflow](#p3_link) (Python, Neural Networks, Tensorflow, Cross Validation)
4. [Predicting Sales from Time Series Data with Autoregressive Models](#p4_link) (R, Time Series Analysis, ARIMA, Vector Autoregressive Models)
5. [Project 5](#p5_link) 


# <a name="p1_link"></a> Deriving Strategic Targets by Analyzing Amazon Customer Reviews

Goal of this project was to gain insights into a certain product category by analyzing reviews for respective products on Amazon. The Dataset was first filtered for heaphone reviews only. 

#### Preprocessing
Before Analysis, the raw review have to be cleaned and transformed into a less noisy version. The Process taken was as follows:

![Diagram1](images/Amazon/preprocessing_diagram.png)

Afterwards, different Analysis approaches were taken. Exploratory Data Analysis and Topic Modeling were combined to identify potenital features to target in order to improve customer satisfaction.

To get a fast view on the dataset, word clouds were created:

![Diagram2](images/Amazon/ngrams.png)

#### Analyzing Brand Position
Through word2vec, the brand position was analayzed. This is useful to identify possible targets for marketing. The calculations were generated through projecting the different word embedding vecctors generated by the word2vec skip-gram network onto the axis between the two analyzed brands, *bose* and *audiotechnica* (See Bolukbasi et al. 2016 for Reference)

![Diagram3](images/Amazon/brand_positioning.png)

For example, Bose is more associated with instrumental, rock and pop music. This is important information and allows the brand to focus exactly these styles of music, improving the customer experience. It is also visible that customer group budget and premium brands, which seems natural.

#### Topic Modeling
Finally, topic modeling is applied to analyze different topics for importance/performance. This way, brand strengths and weaknesses are identified, allowing for meaningful strategic decisions.

For *Latent Dirichlet Analysis (LDA)*, the number of topics *K* is critical. Next to trial-and-error approaches, heuristics like *Bayes Factor* and *Dispersion* exist and are to be considered. From these heuristics, the optimal number of topics was set to *K=8*, as here, Dispersion and the logarithm of BayesFactor both started to converge as visible from the slopes below.

![Diagram3](images/Amazon/bayes_factor_dispersion.png)

Through running the model, the probabilities *δ* and *ω* are generated. Thes probabilities indicate:

- __δ__: Probability of a word belonging to a topic
- __ω__: Probability of a topic being present in a document

Through calculating metrics such as the Lift or using tf-idf, the topics can be named. The individual topics present in the reviews were identified as:

The Top 10 words ranked for the individual topics by null-term lift were:

{:refdef: style="text-align: center;"}
![Diagram5](images/Amazon/wordsrankedbylift.png){:height="80%" width="80%"}
{: refdef}

Therefore, the 8 Topics:

1. Overall satisfaction
2. Gifting
3. Noise cancellation
4. Brand/Model
5. Durability/Customer service
6. Comfort
7. Gaming performance
8. Connectivity/Battery

where derived.

#### Importance/Performance
To assess importance of a topic, K bivariate Linear Regression models were fitted using the standardized __ω__ probabilites as the regressor, predicting the rating score of the review. Through standardization, the regression coefficients become comparable and can be used for the calculation of a topic importance score. Performance was measured by weighing the rating of each review by the probability of the topic appearing in the respective document.

{:refdef: style="text-align: center;"}
![Diagram5](images/Amazon/impeq.png){:height="80%" width="30%"}
![Diagram6](images/Amazon/perfeq.png){:height="80%" width="30%"}
{: refdef}

Performance (x-Axis) was plotted against Importance (y-Axis) for the brand *Bose*. The diagram is shown below.

{:refdef: style="text-align: center;"}
![Diagram7](images/Amazon/imperf.png){:height="80%" width="80%"}
{: refdef}

From this, different strategic targets for *Bose* can be derived:
- Durability and customer support are possible targets for strategic priorization. However, there is likely some bias involved, as unsatisfied customers are more likely to write reviews. Additionally, some of the reviews likely refer to the retailers (Amazon) customer service.
- Gaming seems to be a potential target. Improvements in regard to the headphones gaming performance could drive customer satisfaction
- Battery Life and comfort are not driving ratings very much based on the regression coefficients. However, the are mentioned more likely in reviews that have only average ratings, therefore the headphones could likely be improved in this regard. 

More insights were generated, however, for readability and consiseness, these results are omitted here.

# <a name="p2_link"></a> Using Computer Vision and Machine Learning to Identify Material Defects
This project is an excerpt from my bachelor thesis. Therefore, no completely comprehensive overview is given.
The goal was the identification of material defects under specific conditions:

- only little data is available, as defect detection should be carried out for small production series. During processing, the camera takes more pictures and therefore generates additional data. However, defect detection should be possible right from the start (first few images). This poses a significant challenge.
- deffect detection has to be fast, as online defect detection should be possible. Therefore no extremely complex calculations can be carried out

Under these constraints, classical machine learning approaches are not useful, as CNN's need relatively large amounts of data. To combat this problem, a hybrid approach was taken, using classic computer vision techniques for early defect detection and supplementing this with transfer learning based one class support vector classification later

#### Image Acquisition

The first step in camera based defect detection is image acquisition. Here, an iDS Industry Camera was used. As Edge detection is crucial for detecting surface defects, the right camera parameters are needed when taking pictures. Below, the influence of falsely configured focus (left), low exposure (middle) and correctly configured parameters (right) are shown. It is visible that the edge with the surface defect is only visible in the transformed edge image if the correct parameters are used.

![Diagram3](images/DefectDetection/camera_params.png)

#### Segmentation

Afterwards, the image taken is transformed multiple times:

![Diagram3](images/DefectDetection/SegmentationApproach.png)

This helps to remove any image noise or dirt/metal outside the workpieces contour that would otherwise interfere with defect detection, as this is common especially during processes like milling. The goal of the segmentation is to use the resulting mask to select the relevant image regions only. The resulting segmentation mask of a simple workpiece along with the steps taken to generate it is shown below.

![Diagram3](images/DefectDetection/segmented.png)

#### Positional Correction & Anomaly Detection

Often, there are slight variations in object position during machine operations, which is why for every repetition, the workpiece position is tracked by the bounding box and corrected accordingly. If deviation from the "normal" bounding box is too large, a message is shown to the user so that workpiece position can be corrected.

![Diagram3](images/DefectDetection/learned_bounding_box.png)

#### Defect Detection & Localization

For detecting surface defects, a single reference image on an intact workpiece is needed. Afterwards, the algorithm checks for surface defects using the difference betweeen the edge images. As long as the first process did not produce any surface defects, this approach allows for the detection beginning in the second iteration and is therefore usable for small production series. Below, the defect detection is shown.

![Diagram3](images/DefectDetection/detected_defects.png)

#### Hybrid Approach: Adding Deep Learning

Sometimes, the same workpiece is processed multiple times. If enough data accumulates, an ensemble approach combining machine learning with the established computer vision based approach may prove to be superior. Here, I used a combination of Transfer Learning and a One-Class-Support-Vector-Machine to flag images that fall out of the norm. To reduce complexity, PCA was introduced between the Neural Network output and the SVM. Input to Alexnet were the inverted edge images of the individual workpieces up to the current run.

![Diagram3](images/DefectDetection/mldefects.png)

Using the edge images as inpput proved superior when the data set was still relatively small. The comparison for using edge images as input vs. using the raw images of the workpieces is shown in the graphs below. Relatively recall was achieved, meaning that most anomalies are found, while precision was still comparatively high, leading to reduced rejects of intact workpieces.
 
 ![Diagram3](images/DefectDetection/resultsml.png)
 
Through combining the early, computer vision based defect detection with the machine learning based approach, the detection is able to flag damaged parts in small series production and improve through machine learning if series become larger.
 
# <a name="p3_link"></a> Solving the Kaggle Titanic Challenge using Tensorflow
 
This is a project I did for fun a few years age: I wanted to use a custom neural network to solve the infamous Kaggle Titanic challenge. The full jupyter notebook is available at [Kaggle Titanic challenge using Tensorflow](https://github.com/YannicP/MachineLearning/blob/master/KaggleTitanic/titanic_survival.ipynb)
 
The dataset consists of the records for individual passengers of the titanic and their survival status, found in the respective column.

![Diagram3](images/Titanic/data_table_titanic.png)
 
Before the model can be applied, feature engineering and feature selection were performed. To get an idea what score is acceptable, a logistic regression classifier as well as a random forest classifier were trained using the respective scikit-learn functions.

Some very basic data exploration reveals a few interesting insights:

![Diagram3](images/Titanic/titanicexplore.png)

- Women were a lot more likely to survive, and so were kids. It seems like women and children were indeed evacuated with priority.
- people that payd more, and were passengers in a higher class were more likely to survive.

The Neural Networks structure is as follows:
1. Input Layer: droput rate 0.2
2. Hidden Layer 1: ELU activation, L2 regularization (λ = 0.01), dropout rate 0.5 & batch normalization
3. Hidden Layer 2: ELU activation, L2 regularization (λ = 0.01), dropout rate 0.15 & batch normalization

For initialization, xavier initialization was used.
The tensorflow code creating the model is shown below.
 ```
 with tf.name_scope("neural_network"):
    xavier_init = tf.contrib.layers.xavier_initializer()
    
    X_drop = tf.layers.dropout(X, dropout_rate_0, training=training)
    
    hidden_1 = tf.layers.dense(X_drop, n_hidden_1, kernel_regularizer=tf.contrib.layers.l2_regularizer(0.01), kernel_initializer=xavier_init, name="hidden_1")
    bn1 = tf.layers.batch_normalization(hidden_1, training=training, momentum=0.9)
    bn1_act = tf.nn.elu(bn1)
    hidden_1_drop = tf.layers.dropout(bn1_act, dropout_rate_1, training=training)
    
    hidden_2 = tf.layers.dense(hidden_1_drop, n_hidden_2, kernel_regularizer=tf.contrib.layers.l2_regularizer(0.01), kernel_initializer=xavier_init, name="hidden_2")
    bn2 = tf.layers.batch_normalization(hidden_2, training=training, momentum=0.9)
    bn2_act = tf.nn.elu(bn2)
    hidden_2_drop = tf.layers.dropout(bn2_act, dropout_rate_2, training=training)
    
    logits = tf.layers.dense(hidden_2_drop, n_outputs, kernel_initializer=xavier_init, name="outputs")
 ```
 
# <a name="p4_link"></a> Predicting Sales from Time Series Data with Autoregressive Models
 
This project deals with forecasting sales for an internet startup based on a given time series. The time series is shown in the following diagram:
 
![Diagram](images/SalesPrediction/monthly_sales.png)
 
It is clearly visible that the companies sales are clearly dependent on the season. In November and December, Sales are exceptionally high compared to the rest of the year.
In order to apply forecasting to this time series, the data first needs to be stationary. This is also indicated by the Dicky-Fuller test, where the null hypothesis is rejected at α = 0.05

![Diagram](images/SalesPrediction/adftest.png)
 
References:

[Bolukbasi et al. 2016] Tolga Bolukbasi and Kai-Wei Chang and James Zou and Venkatesh Saligrama and Adam Kalai. *Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings*. 2016
