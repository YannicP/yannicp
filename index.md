## Some of my Projects:

This document gives some insights into some of my past projects, with a focus on variety.

### Contents:
You can click the project to jump straight to the section.

1. [Deriving Strategic Targets by Analyzing Amazon Customer Reviews](#p1_link) (R, text mining, Word2Vec, topic modeling, LDA)  
2. [Using Computer Vision and Machine Learning to Identify Material Defects](#p2_link) (Matlab, Image Transformations, CNN, Transfer Learning, One-Class-SVM)
3. [Predicting Sales from Time Series Data with Autoregressive Models](#p3_link) (R, Time Series Analysis, ARIMA, Grid Search)
4. [Solving the Kaggle Titanic Challenge using Tensorflow](#p4_link) (Python, Neural Networks, Tensorflow, Cross Validation)
5. [Project 5](#p5_link) 


# <a name="p1_link"></a> Deriving Strategic Targets by Analyzing Amazon Customer Reviews

Goal of this project was to gain insights into a certain product category by analyzing reviews for respective products on Amazon. The Dataset was first filtered for heaphone reviews only. 

#### Preprocessing
Before Analysis, the raw review have to be cleaned and transformed into a less noisy version. The Process taken was as follows:

![Diagram1](images/Amazon/preprocessing_diagram.png)

Afterwards, different Analysis approaches were taken. Exploratory Data Analysis and Topic Modeling were combined to identify potenital features to target in order to improve customer satisfaction.

To get a fast view on the dataset, word clouds were created:

![Diagram2](images/Amazon/ngrams.png)

#### Analyzing Brand Position
Through word2vec, the brand position was analayzed. This is useful to identify possible targets for marketing. The calculations were generated through projecting the different word embedding vecctors generated by the word2vec skip-gram network onto the axis between the two analyzed brands, *bose* and *audiotechnica* (See Bolukbasi et al. 2016 for Reference)

![Diagram3](images/Amazon/brand_positioning.png)

For example, Bose is more associated with instrumental, rock and pop music. This is important information and allows the brand to focus exactly these styles of music, improving the customer experience. It is also visible that customer group budget and premium brands, which seems natural.

#### Topic Modeling
Finally, topic modeling is applied to analyze different topics for importance/performance. This way, brand strengths and weaknesses are identified, allowing for meaningful strategic decisions.

For *Latent Dirichlet Analysis (LDA)*, the number of topics *K* is critical. Next to trial-and-error approaches, heuristics like *Bayes Factor* and *Dispersion* exist and are to be considered. From these heuristics, the optimal number of topics was set to *K=8*, as here, Dispersion and the logarithm of BayesFactor both started to converge as visible from the slopes below.

![Diagram3](images/Amazon/bayes_factor_dispersion.png)

Through running the model, the probabilities *δ* and *ω* are generated. Thes probabilities indicate:

- __δ__: Probability of a word belonging to a topic
- __ω__: Probability of a topic being present in a document

Through calculating metrics such as the Lift or using tf-idf, the topics can be named. The individual topics present in the reviews were identified as:

The Top 10 words ranked for the individual topics by null-term lift were:

{:refdef: style="text-align: center;"}
![Diagram5](images/Amazon/wordsrankedbylift.png){:height="80%" width="80%"}
{: refdef}

Therefore, the 8 Topics:

1. Overall satisfaction
2. Gifting
3. Noise cancellation
4. Brand/Model
5. Durability/Customer service
6. Comfort
7. Gaming performance
8. Connectivity/Battery

where derived.

#### Importance/Performance
To assess importance of a topic, K bivariate Linear Regression models were fitted using the standardized __ω__ probabilites as the regressor, predicting the rating score of the review. Through standardization, the regression coefficients become comparable and can be used for the calculation of a topic importance score. Performance was measured by weighing the rating of each review by the probability of the topic appearing in the respective document.

{:refdef: style="text-align: center;"}
![Diagram5](images/Amazon/impeq.png){:height="80%" width="30%"}
![Diagram6](images/Amazon/perfeq.png){:height="80%" width="30%"}
{: refdef}

Performance (x-Axis) was plotted against Importance (y-Axis) for the brand *Bose*. The diagram is shown below.

{:refdef: style="text-align: center;"}
![Diagram7](images/Amazon/imperf.png){:height="80%" width="80%"}
{: refdef}

From this, different strategic targets for *Bose* can be derived:
- Durability and customer support are possible targets for strategic priorization. However, there is likely some bias involved, as unsatisfied customers are more likely to write reviews. Additionally, some of the reviews likely refer to the retailers (Amazon) customer service.
- Gaming seems to be a potential target. Improvements in regard to the headphones gaming performance could drive customer satisfaction
- Battery Life and comfort are not driving ratings very much based on the regression coefficients. However, the are mentioned more likely in reviews that have only average ratings, therefore the headphones could likely be improved in this regard. 

More insights were generated, however, for readability and consiseness, these results are omitted here.

# <a name="p2_link"></a> Using Computer Vision and Machine Learning to Identify Material Defects
This project is an excerpt from my bachelor thesis. Therefore, no completely comprehensive overview is given.
The goal was the identification of material defects under specific conditions:

- only little data is available, as defect detection should be carried out for small production series. During processing, the camera takes more pictures and therefore generates additional data. However, defect detection should be possible right from the start (first few images). This poses a significant challenge.
- deffect detection has to be fast, as online defect detection should be possible. Therefore no extremely complex calculations can be carried out

Under these constraints, classical machine learning approaches are not useful, as CNN's need relatively large amounts of data. To combat this problem, a hybrid approach was taken, using classic computer vision techniques for early defect detection and supplementing this with transfer learning based one class support vector classification later

#### Image Acquisition

The first step in camera based defect detection is image acquisition. Here, an iDS Industry Camera was used. As Edge detection is crucial for detecting surface defects, the right camera parameters are needed when taking pictures. Below, the influence of falsely configured focus (left), low exposure (middle) and correctly configured parameters (right) are shown. It is visible that the edge with the surface defect is only visible in the transformed edge image if the correct parameters are used.

![Diagram3](images/DefectDetection/camera_params.png)

#### Segmentation

Afterwards, the image taken is transformed multiple times:

![Diagram3](images/DefectDetection/SegmentationApproach.png)

This helps to remove any image noise or dirt/metal outside the workpieces contour that would otherwise interfere with defect detection, as this is common especially during processes like milling. The goal of the segmentation is to use the resulting mask to select the relevant image regions only. The resulting segmentation mask of a simple workpiece along with the steps taken to generate it is shown below.

![Diagram3](images/DefectDetection/segmented.png)

#### Positional Correction & Anomaly Detection

Often, there are slight variations in object position during machine operations, which is why for every repetition, the workpiece position is tracked by the bounding box and corrected accordingly. If deviation from the "normal" bounding box is too large, a message is shown to the user so that workpiece position can be corrected.

![Diagram3](images/DefectDetection/learned_bounding_box.png)

#### Defect Detection & Localization

For detecting surface defects, a single reference image on an intact workpiece is needed. Afterwards, the algorithm checks for surface defects using the difference betweeen the edge images. As long as the first process did not produce any surface defects, this approach allows for the detection beginning in the second iteration and is therefore usable for small production series. Below, the defect detection is shown.

![Diagram3](images/DefectDetection/detected_defects.png)

#### Hybrid Approach: Adding Deep Learning

Sometimes, the same workpiece is processed multiple times. If enough data accumulates, an ensemble approach combining machine learning with the established computer vision based approach may prove to be superior. Here, I used a combination of Transfer Learning and a One-Class-Support-Vector-Machine to flag images that fall out of the norm. To reduce complexity, PCA was introduced between the Neural Network output and the SVM. Input to Alexnet were the inverted edge images of the individual workpieces up to the current run.

![Diagram3](images/DefectDetection/mldefects.png)

Using the edge images as inpput proved superior when the data set was still relatively small. The comparison for using edge images as input vs. using the raw images of the workpieces is shown in the graphs below. Relatively recall was achieved, meaning that most anomalies are found, while precision was still comparatively high, leading to reduced rejects of intact workpieces.
 
 ![Diagram3](images/DefectDetection/resultsml.png)
 
Through combining the early, computer vision based defect detection with the machine learning based approach, the detection is able to flag damaged parts in small series production and improve through machine learning if series become larger.
 
# <a name="p3_link"></a> Predicting Sales from Time Series Data with Autoregressive Models
 
This project deals with forecasting sales for an internet startup based on a given time series. The time series is shown in the following diagram:
 
![Diagram](images/SalesPrediction/monthly_sales.png)
 
It is clearly visible that the companies sales are clearly dependent on the season. In November and December, Sales are exceptionally high compared to the rest of the year.
In order to apply forecasting to this time series, the data first needs to be stationary. This is also indicated by the Dicky-Fuller test, where the null hypothesis is rejected at α = 0.05

![Diagram](images/SalesPrediction/adftest.png)

#### Making the Data Stationary
Through applying the logarithm (which is equal to a Box-Cox-Transform with λ = 0), possible heteroskedacity is removed. This yields a the following time series:

![Diagram](images/SalesPrediction/log_trans.png)

The data is still not stationary, as there is a trend present (this can also be confirmed by runnning another DF test). Therefore, detrending via differencing is applied:

![Diagram](images/SalesPrediction/log_trans_diff.png)

Following this, the seasonal component is remaining. Taking seasonal differences (with lag=12) is one way to remove this effect. The result (and final stationary time series) looks like this:

![Diagram](images/SalesPrediction/seasonalcorrf.png)


#### Using (S)ARIMA to Model Sales

With a stationary time series being present, the next step is the creation of a forecasting model. Commonly, autoregressive models are used for this task.
The creation of a ARIMA (Auto-Regressive Integrated Moving Average) model requires the selection of specific parameters *p* and *q* that tell it how many autoregressive lags (correlation with previous values) and moving average lags (correlation of the error terms) should be taken into account.

This parameter selection can be achieved by taking a look at the ACF and PACF funtions for the time series:

![Diagram](images/SalesPrediction/acf_pacf.png)
 
- To identify a possible value for *p*, one looks at which lag the PACF function (right) first crosses the significance threshold. This is slightly subjective, so here, values of 2 or 4 are possible choices
- To identify the value for *q*, the same methodology is employed, this time looking at the ACF function. Here, values of 1 (to keep complexity lower) or 2 are possible choices for *q*
- for the seasonal component, the values *p* and *q* have to be chosen as well. looking at their respective ACF and PACF funtions (not shown here), reveals *p* = 12 (obviously, due to seasonality) and *q* = 0.

Setting up the model with these parameters yield the respective coefficients that tell how the individual values are correlated:

![Diagram](images/SalesPrediction/arimaoutput.png)

Using the model, predictions can be made for the sales: Here, only one year is used for training, while the rest of the timeframe acts as a test set:

![Diagram](images/SalesPrediction/arima_preds.png)

To support the chosen parameters, a grid search was run, using the values from the year 2006-2011 as training set, predicting the values for the year 2012.
The parameter combinations and the respective RMSE values (This time for the log transformed sales data) are shown for the grid:

![Diagram](images/SalesPrediction/training_rmses.png)

This further supports the parameter choice.
The predictions for the log transformed sales for 2012 are shown additionally - it is visible that the general shape is compatible, but some slight differences between prediction and ground truth exist.

![Diagram](images/SalesPrediction/actual_vs_predsales.png)

# <a name="p4_link"></a> Solving the Kaggle Titanic Challenge using Tensorflow
 
This is a project I did for fun a few years age: I wanted to use a custom neural network to solve the infamous Kaggle Titanic challenge. The full jupyter notebook is available at [Kaggle Titanic challenge using Tensorflow](https://github.com/YannicP/MachineLearning/blob/master/KaggleTitanic/titanic_survival.ipynb)
 
The dataset consists of the records for individual passengers of the titanic and their survival status, found in the respective column.

![Diagram3](images/Titanic/data_table_titanic.png)
 
Before the model can be applied, feature engineering and feature selection were performed. To get an idea what score is acceptable, a logistic regression classifier as well as a random forest classifier were trained using the respective scikit-learn functions.

Some very basic data exploration reveals a few interesting insights:

![Diagram3](images/Titanic/titanicexplore.png)

- Women were a lot more likely to survive, and so were kids. It seems like women and children were indeed evacuated with priority.
- people that payd more, and were passengers in a higher class were more likely to survive.

#### Setting up and training a heaviliy regularized Neural Network
Neural Networks have the disadvantage of overfitting if no countermeasures are taken. Especially when little data is present (as is the case here), they tend to follow the training data too closely, making overfitting a daunting challeng. To combat this, there are various techniques available:

- Regularizing the loss function: Penalizing the loss if the networks weights get too large
- Using dropout: Deactivating random connections during training does not allow the network to lean on specific features too heavily
- Early stopping: Stopping training early can lead to better results, as the weights have not yet converged to values that model the training set optimally

The Neural Networks structure is as follows:
1. Input Layer: droput rate 0.2
2. Hidden Layer 1: ELU activation, L2 regularization (λ = 0.01), dropout rate 0.5 & batch normalization
3. Hidden Layer 2: ELU activation, L2 regularization (λ = 0.01), dropout rate 0.15 & batch normalization

For initialization, xavier initialization was used.
Setting up the network in tensorflow is fairly straightforward:
 ```
 with tf.name_scope("neural_network"):
    xavier_init = tf.contrib.layers.xavier_initializer()
    
    X_drop = tf.layers.dropout(X, dropout_rate_0, training=training)
    
    hidden_1 = tf.layers.dense(
                     X_drop, 
                     n_hidden_1, 
                     kernel_regularizer=tf.contrib.layers.l2_regularizer(0.01), 
                     kernel_initializer=xavier_init, 
                     name="hidden_1"
                     )
    bn1 = tf.layers.batch_normalization(
                     hidden_1, 
                     training=training, 
                     momentum=0.9
                     )
    bn1_act = tf.nn.elu(bn1)
    hidden_1_drop = tf.layers.dropout(
                     bn1_act, 
                     dropout_rate_1, 
                     training=training
                     )
    
    hidden_2 = tf.layers.dense(
                     hidden_1_drop, 
                     n_hidden_2, 
                     kernel_regularizer=tf.contrib.layers.l2_regularizer(0.01), 
                     kernel_initializer=xavier_init, name="hidden_2"
                     )
    bn2 = tf.layers.batch_normalization(
                     hidden_2, 
                     training=training, 
                     momentum=0.9
                     )
    bn2_act = tf.nn.elu(bn2)
    hidden_2_drop = tf.layers.dropout(
                    bn2_act, 
                    dropout_rate_2, 
                    training=training
                    )
    
    logits = tf.layers.dense(
                    hidden_2_drop, 
                    n_outputs, 
                    kernel_initializer=xavier_init, 
                    name="outputs"
                    )
 ```
 The loss funtion used is basig cross entropy, while training is carried out using adaptive moment estimation:
 
 ```
 with tf.name_scope("loss"):
    xentropy = tf.nn.sigmoid_cross_entropy_with_logits(
                    labels=y, 
                    logits=logits
                    )
    loss = tf.reduce_mean(xentropy, name="loss") + tf.losses.get_regularization_loss()
    
    
with tf.name_scope("train"):
    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
    training_op = optimizer.minimize(loss)
 ```
 
#### Evaluation using 10-fold cross validation
 
To test for generalizability, 10-fold cross validation was implemented. This has the advantage that all of the data available can be used for training afterwards and no skew due to the selection of a specific test set is introduced ( no overfitting the test set)
 Training on ten folds yields the following accuracies:
 
 | Fold  | Training Loss | Validation Loss|
| ------------- | ------------- | ------------- |
| 1  | 0.845 | 0.833 |
| 2  | 0.843 | 0.88 |
| 3  | 0.849 | 0.78 |
| 4  | 0.838 | 0.844 |
| 5  | 0.851 | 0.844 |
| 6  | 0.844 | 0.833 |
| 7  | 0.843 | 0.81 |
| 8  | 0.839 | 0.833 |
| 9  | 0.83 | 0.889 |
| 10  | 0.846 | 0.863 |

This shows that using regularization, dropout, batch normalization and a relatively shallow structure with only two hidden layers allowed the network to generalize pretty well. Even tough neural networks are not necessarily optimal for this kind of task, a heavy regularized network allows for good predictions, as the model scored roughly in the 95th percentile of all models (in 2018).

References:

[Bolukbasi et al. 2016] Tolga Bolukbasi and Kai-Wei Chang and James Zou and Venkatesh Saligrama and Adam Kalai. *Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings*. 2016
